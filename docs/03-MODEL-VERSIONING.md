# JAVIS 모델 버전업 전략

## 핵심 개념

### 나만의 모델 = 베이스 + 어댑터

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   나만의 JAVIS 모델                                          │
│                                                             │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  베이스 모델 (오픈소스)                               │   │
│   │  Qwen2.5-7B-Instruct                                │   │
│   │  - 기본 언어 능력                                    │   │
│   │  - 추론 능력                                        │   │
│   │  - 일반 지식                                        │   │
│   └─────────────────────────────────────────────────────┘   │
│                          +                                  │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  LoRA 어댑터 (내가 학습)                             │   │
│   │  javis-lora-v0.1.safetensors                        │   │
│   │  - 나의 말투/스타일                                  │   │
│   │  - 특화된 행동 패턴                                  │   │
│   │  - 도구 사용 방식                                   │   │
│   └─────────────────────────────────────────────────────┘   │
│                          =                                  │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  JAVIS v0.1                                         │   │
│   │  나만의 AI 모델                                      │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 버전업 방법

### 방법 1: 데이터 추가 파인튜닝

**언제**: 모델 행동을 개선하고 싶을 때

```
v0.1 (데이터 100개)
    │
    │  + 새로운 데이터 100개 추가
    │  + 기존 나쁜 데이터 제거
    │  + 품질 개선
    │
    ▼
v0.2 (데이터 180개, 정제됨)
    │
    │  + 도구 사용 데이터 추가
    │  + 복잡한 시나리오 추가
    │
    ▼
v0.3 (데이터 300개)
```

**학습 방식**:
- 전체 데이터로 처음부터 재학습 (권장)
- 또는 이전 어댑터 위에 추가 학습 (Continual)

### 방법 2: 베이스 모델 업그레이드

**언제**: 기본 능력(추론, 지식)을 향상시키고 싶을 때

```
JAVIS on Qwen2.5-7B
    │
    │  [새로운 베이스 모델로 교체]
    │  - 더 큰 모델: 14B, 32B
    │  - 또는 새 버전: Qwen3.0 (출시 시)
    │
    ▼
JAVIS on Qwen2.5-14B (새로 파인튜닝)
    │
    │  기존 학습 데이터 재사용
    │  베이스 능력 향상 + 나의 스타일 유지
    │
    ▼
더 똑똑한 JAVIS
```

**주의사항**:
- 베이스 교체 시 어댑터 재학습 필요
- 학습 데이터는 그대로 재사용 가능
- 새 베이스 + 기존 데이터 = 새 어댑터

### 방법 3: 고급 학습 기법 적용

**언제**: 더 정교한 행동 제어가 필요할 때

```
v1.0 (SFT만 적용)
    │
    │  [DPO 적용]
    │  좋은 응답 vs 나쁜 응답 비교 학습
    │
    ▼
v1.1 (SFT + DPO)
    │
    │  [특정 능력 강화]
    │  코딩 데이터 집중 학습
    │
    ▼
v1.2 (SFT + DPO + 코딩 특화)
```

---

## 학습 기법 상세

### SFT (Supervised Fine-Tuning)

기본 파인튜닝 방식

```json
{
  "messages": [
    {"role": "system", "content": "너는 JAVIS다."},
    {"role": "user", "content": "오늘 할 일 정리해줘"},
    {"role": "assistant", "content": "오늘 할 일 목록입니다:\n1. ..."}
  ]
}
```

**특징**:
- 가장 기본적인 방식
- "이렇게 응답해라" 직접 학습
- 빠르고 간단

### DPO (Direct Preference Optimization)

선호도 기반 학습

```json
{
  "prompt": "코드 리뷰해줘",
  "chosen": "코드를 분석해보겠습니다.\n\n1. 보안 이슈: ...",
  "rejected": "네, 코드 리뷰 해드릴게요~ 일단 보시면요..."
}
```

**특징**:
- "이게 더 좋다" 비교 학습
- 미묘한 품질 차이 학습 가능
- SFT 후 적용 (2단계)

### RLHF (Reinforcement Learning from Human Feedback)

강화학습 기반 (고급)

```
사용자 피드백 → 리워드 모델 학습 → 강화학습

복잡하고 비용이 많이 들어 개인 프로젝트에선 DPO가 더 현실적
```

---

## 버전 관리 체계

### 디렉토리 구조

```
models/
├── adapters/
│   ├── javis-v0.1/
│   │   ├── adapter_config.json
│   │   ├── adapter_model.safetensors
│   │   └── README.md
│   │
│   ├── javis-v0.2/
│   │   └── ...
│   │
│   └── javis-v1.0/
│       └── ...
│
├── configs/
│   ├── training_v0.1.yaml
│   ├── training_v0.2.yaml
│   └── ...
│
└── current -> adapters/javis-v1.0  (현재 사용 버전)
```

### 버전 네이밍

```
javis-v{major}.{minor}[-{tag}]

예시:
- javis-v0.1        # 첫 버전
- javis-v0.2        # 데이터 추가
- javis-v0.3-tools  # 도구 특화
- javis-v1.0        # 첫 안정 버전
- javis-v1.1-dpo    # DPO 적용
- javis-v2.0        # 베이스 모델 업그레이드
```

### 메타데이터 기록

```yaml
# models/adapters/javis-v0.2/metadata.yaml
version: "0.2"
created_at: "2024-01-15"
base_model: "Qwen/Qwen2.5-7B-Instruct"

training:
  method: "QLoRA"
  epochs: 3
  learning_rate: 0.0002
  data_count: 180

data:
  sources:
    - conversations_general.jsonl (80)
    - conversations_tools.jsonl (50)
    - conversations_coding.jsonl (50)

changes_from_previous:
  - "도구 호출 데이터 50개 추가"
  - "품질 낮은 응답 20개 제거"
  - "시스템 프롬프트 개선"

evaluation:
  tool_accuracy: 0.85
  style_consistency: 0.90
  user_satisfaction: 4.2/5
```

---

## 학습 데이터 관리

### 데이터 포맷 (ChatML)

```jsonl
{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
```

### 데이터 버전 관리

```
data/
├── training/
│   ├── v0.1/
│   │   ├── conversations.jsonl
│   │   └── manifest.json
│   │
│   ├── v0.2/
│   │   ├── conversations.jsonl    # v0.1 데이터 포함 + 신규
│   │   ├── manifest.json
│   │   └── changelog.md
│   │
│   └── v0.3/
│       └── ...
│
├── feedback/
│   ├── good_responses.jsonl       # 👍 받은 응답
│   ├── bad_responses.jsonl        # 👎 받은 응답 (수정 전)
│   └── corrected_responses.jsonl  # 수정된 응답
│
└── raw/
    └── chat_logs.jsonl            # 전체 대화 로그
```

### 데이터 품질 관리

```python
# 데이터 품질 체크리스트
quality_criteria = {
    "length": "응답이 너무 짧거나 길지 않은가?",
    "relevance": "질문에 적절히 답변하는가?",
    "style": "원하는 말투/스타일인가?",
    "accuracy": "정보가 정확한가?",
    "format": "형식이 일관적인가?",
    "safety": "유해한 내용은 없는가?",
}

# 품질 점수: 1-5
# 4점 이상만 학습 데이터로 사용
```

---

## 버전업 파이프라인

### 자동화된 버전업 프로세스

```
┌─────────────────────────────────────────────────────────────┐
│                    버전업 파이프라인                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  [1. 데이터 수집]                                            │
│      │                                                      │
│      │  • 일상 사용 중 피드백 축적                            │
│      │  • 👍 좋은 응답 저장                                  │
│      │  • 👎 나쁜 응답 → 수정 후 저장                        │
│      │                                                      │
│      ▼                                                      │
│  [2. 데이터 검증] (자동)                                     │
│      │                                                      │
│      │  • 포맷 검증                                         │
│      │  • 중복 제거                                         │
│      │  • 품질 필터링                                       │
│      │                                                      │
│      ▼                                                      │
│  [3. 학습 트리거]                                            │
│      │                                                      │
│      │  조건: 신규 데이터 50개 이상                           │
│      │  또는: 수동 트리거                                    │
│      │                                                      │
│      ▼                                                      │
│  [4. 파인튜닝 실행] (클라우드)                                │
│      │                                                      │
│      │  • 데이터 업로드                                     │
│      │  • QLoRA 학습 (1-2시간)                              │
│      │  • 체크포인트 저장                                   │
│      │                                                      │
│      ▼                                                      │
│  [5. 자동 평가]                                              │
│      │                                                      │
│      │  • 테스트 세트로 평가                                 │
│      │  • 이전 버전과 비교                                   │
│      │  • 성능 리포트 생성                                   │
│      │                                                      │
│      ▼                                                      │
│  [6. 배포 결정]                                              │
│      │                                                      │
│      │  성능 향상? → 자동 배포 또는 수동 승인                  │
│      │  성능 저하? → 롤백, 데이터 검토                        │
│      │                                                      │
│      ▼                                                      │
│  [7. 프로덕션 배포]                                          │
│      │                                                      │
│      │  • RunPod 엔드포인트 업데이트                         │
│      │  • current 심볼릭 링크 업데이트                       │
│      │  • 버전 태그 기록                                    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 장기 발전 로드맵

### 모델 성장 경로

```
Year 1
───────────────────────────────────────────────────────

[Q1] 기반 구축
     ├── Qwen2.5-7B 기반 v0.1 ~ v0.5
     ├── 기본 대화 + 도구 사용
     └── 데이터 500개 축적

[Q2] 안정화
     ├── v1.0 출시 (첫 안정 버전)
     ├── DPO 적용
     └── 데이터 1000개

[Q3] 확장
     ├── Qwen2.5-14B로 업그레이드 (v2.0)
     ├── RAG 통합 학습
     └── 데이터 2000개

[Q4] 고도화
     ├── 특화 모델 실험 (코딩, 분석)
     ├── 멀티태스크 학습
     └── v3.0


Year 2+
───────────────────────────────────────────────────────

     ├── 32B+ 모델로 확장
     ├── 새로운 베이스 모델 평가 (Qwen3, Llama4 등)
     ├── 멀티모달 통합
     └── 완전 자율 에이전트
```

---

## 핵심 원칙

```
1. 데이터가 왕이다
   - 모델 능력의 80%는 데이터 품질에서 온다
   - 쓰레기 데이터 100개보다 좋은 데이터 10개가 낫다

2. 점진적 개선
   - 한 번에 너무 많이 바꾸지 않는다
   - 작은 변화 → 평가 → 반영 사이클

3. 항상 비교 가능하게
   - 이전 버전과 새 버전을 항상 비교
   - 객관적 평가 지표 유지

4. 롤백 가능하게
   - 모든 버전 보관
   - 문제 시 즉시 이전 버전으로 복구
```
