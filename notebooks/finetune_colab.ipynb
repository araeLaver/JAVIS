{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# JAVIS Fine-tuning (Google Colab)\n",
        "\n",
        "Qwen2.5-7B-Instruct QLoRA 파인튜닝\n",
        "\n",
        "**중요: 셀을 순서대로 실행하세요!**\n",
        "\n",
        "**필요사항:**\n",
        "- Google Colab (무료 T4 GPU)\n",
        "- HuggingFace 토큰\n",
        "- 학습 데이터 (JSONL)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 환경 설정"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] GPU 확인\n",
        "!nvidia-smi"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [2] 패키지 설치 (2-3분 소요)\n",
        "!pip install -q torch transformers datasets peft trl bitsandbytes accelerate scipy"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [3] 전체 import 및 설정\n",
        "import json\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from datasets import Dataset\n",
        "from google.colab import files\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# 설정값\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "OUTPUT_DIR = \"./javis-adapter\"\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 2e-4\n",
        "LORA_R = 64\n",
        "LORA_ALPHA = 16\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "\n",
        "print(\"Import 완료!\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [4] HuggingFace 로그인\n",
        "# https://huggingface.co/settings/tokens 에서 토큰 발급\n",
        "HF_TOKEN = \"hf_xxxxxxxxxxxxxxxxxxxxx\"  # @param {type:\"string\"}\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 학습 데이터 업로드"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# [5] JSONL 파일 업로드\n",
        "# 로컬에서 export한 conversations_xxx.jsonl 파일 선택\n",
        "print(\"conversations_xxx.jsonl 파일을 선택하세요\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "TRAINING_DATA = list(uploaded.keys())[0]\n",
        "print(f\"\\n업로드 완료: {TRAINING_DATA}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [6] 데이터 로드 및 확인\n",
        "def load_training_data(path):\n",
        "    conversations = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                conversations.append(json.loads(line))\n",
        "    return Dataset.from_list(conversations)\n",
        "\n",
        "dataset = load_training_data(TRAINING_DATA)\n",
        "print(f\"로드된 대화 수: {len(dataset)}\")\n",
        "print(f\"\\n첫 번째 대화 예시:\")\n",
        "print(json.dumps(dataset[0], indent=2, ensure_ascii=False))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 모델 로드"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# [7] 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# 데이터 포맷팅\n",
        "def format_conversation(example):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example['messages'],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = dataset.map(format_conversation, remove_columns=dataset.column_names)\n",
        "print(\"토크나이저 로드 완료\")\n",
        "print(f\"\\n포맷된 예시:\\n{dataset[0]['text'][:300]}...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [8] 모델 로드 (5-10분 소요)\n",
        "print(\"모델 로딩 중... (5-10분 걸립니다)\")\n",
        "\n",
        "# 4-bit 양자화 설정\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(\"모델 로드 완료!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [9] LoRA 설정\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "trainable, total = model.get_nb_trainable_parameters()\n",
        "print(f\"학습 파라미터: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 학습 실행"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# [10] 학습 시작 (1-2시간 소요)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"학습 시작! (데이터 양에 따라 1-2시간 소요)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"학습 완료!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 저장 및 다운로드"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# [11] Adapter 저장\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# 메타데이터 저장\n",
        "metadata = {\n",
        "    \"base_model\": BASE_MODEL,\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"training_config\": {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"lora_r\": LORA_R,\n",
        "        \"lora_alpha\": LORA_ALPHA,\n",
        "    },\n",
        "    \"dataset_size\": len(dataset),\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/metadata.json\", 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"저장 완료!\")\n",
        "!ls -la {OUTPUT_DIR}"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [12] ZIP으로 다운로드\n",
        "!zip -r javis-adapter.zip {OUTPUT_DIR}\n",
        "files.download('javis-adapter.zip')\n",
        "\n",
        "print(\"\\n다운로드 완료! 이 파일을 로컬에 저장하세요.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. (선택) HuggingFace 업로드"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# [13] HuggingFace Hub에 업로드 (선택사항)\n",
        "HF_USERNAME = \"your-username\"  # @param {type:\"string\"}\n",
        "REPO_NAME = \"javis-adapter-v1\"  # @param {type:\"string\"}\n",
        "\n",
        "model.push_to_hub(f\"{HF_USERNAME}/{REPO_NAME}\")\n",
        "tokenizer.push_to_hub(f\"{HF_USERNAME}/{REPO_NAME}\")\n",
        "\n",
        "print(f\"업로드 완료: https://huggingface.co/{HF_USERNAME}/{REPO_NAME}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 테스트"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# [14] 파인튜닝된 모델 테스트\n",
        "test_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"안녕, 넌 누구야?\"}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"테스트 응답:\")\n",
        "print(\"=\"*50)\n",
        "print(response)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
