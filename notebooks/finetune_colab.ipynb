{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# JAVIS Fine-tuning (Google Colab)\n",
        "\n",
        "Qwen2.5-7B-Instruct QLoRA 파인튜닝\n",
        "\n",
        "**필요사항:**\n",
        "- Google Colab (무료 T4 GPU 또는 유료 A100)\n",
        "- HuggingFace 토큰 (Qwen 모델 다운로드용)\n",
        "- 학습 데이터 (JSONL)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 환경 설정"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 확인\n",
        "!nvidia-smi"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 설치\n",
        "!pip install -q torch transformers datasets peft trl bitsandbytes accelerate scipy"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace 로그인\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 여기에 HuggingFace 토큰 입력\n",
        "HF_TOKEN = \"hf_xxxxxxxxxxxxxxxxxxxxx\"  # @param {type:\"string\"}\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 학습 데이터 업로드"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive 마운트 (데이터 업로드용)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 또는 직접 업로드\n",
        "from google.colab import files\n",
        "\n",
        "print(\"conversations_xxx.jsonl 파일을 업로드하세요\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 업로드된 파일명\n",
        "TRAINING_DATA = list(uploaded.keys())[0]\n",
        "print(f\"업로드된 파일: {TRAINING_DATA}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 모델 및 데이터 로드"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# 설정\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "OUTPUT_DIR = \"./javis-adapter\"\n",
        "\n",
        "# 하이퍼파라미터\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 2  # T4는 메모리 작으니까 2\n",
        "LEARNING_RATE = 2e-4\n",
        "LORA_R = 64\n",
        "LORA_ALPHA = 16\n",
        "MAX_SEQ_LENGTH = 1024  # T4는 1024로"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드\n",
        "def load_training_data(path):\n",
        "    conversations = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                conversations.append(json.loads(line))\n",
        "    return Dataset.from_list(conversations)\n",
        "\n",
        "dataset = load_training_data(TRAINING_DATA)\n",
        "print(f\"로드된 대화 수: {len(dataset)}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# 데이터 포맷팅\n",
        "def format_conversation(example):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example['messages'],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = dataset.map(format_conversation, remove_columns=dataset.column_names)\n",
        "print(\"데이터 포맷팅 완료\")\n",
        "print(f\"예시:\\n{dataset[0]['text'][:500]}...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-bit 양자화 설정\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# 모델 로드\n",
        "print(\"모델 로딩 중... (몇 분 걸림)\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "print(\"모델 로드 완료\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA 설정\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "trainable, total = model.get_nb_trainable_parameters()\n",
        "print(f\"학습 파라미터: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 학습 실행"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=True,  # 메모리 절약\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "print(\"학습 시작!\")\n",
        "trainer.train()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 모델 저장"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapter 저장\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Adapter 저장 완료: {OUTPUT_DIR}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메타데이터 저장\n",
        "from datetime import datetime\n",
        "\n",
        "metadata = {\n",
        "    \"base_model\": BASE_MODEL,\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"training_config\": {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"lora_r\": LORA_R,\n",
        "        \"lora_alpha\": LORA_ALPHA,\n",
        "    },\n",
        "    \"dataset_size\": len(dataset),\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/metadata.json\", 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"메타데이터 저장 완료\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 목록 확인\n",
        "!ls -la {OUTPUT_DIR}"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 다운로드 또는 HuggingFace 업로드"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 로컬로 다운로드\n",
        "!zip -r javis-adapter.zip {OUTPUT_DIR}\n",
        "files.download('javis-adapter.zip')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 또는 HuggingFace Hub에 업로드\n",
        "HF_USERNAME = \"your-username\"  # @param {type:\"string\"}\n",
        "REPO_NAME = \"javis-adapter-v1\"  # @param {type:\"string\"}\n",
        "\n",
        "model.push_to_hub(f\"{HF_USERNAME}/{REPO_NAME}\")\n",
        "tokenizer.push_to_hub(f\"{HF_USERNAME}/{REPO_NAME}\")\n",
        "\n",
        "print(f\"업로드 완료: https://huggingface.co/{HF_USERNAME}/{REPO_NAME}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 테스트 추론"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 파인튜닝된 모델로 테스트\n",
        "from peft import PeftModel\n",
        "\n",
        "# Base 모델 다시 로드\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Adapter 적용\n",
        "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "\n",
        "# 테스트\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"안녕, 넌 누구야?\"}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7, do_sample=True)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"응답:\")\n",
        "print(response)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
